{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Stocker\n",
    "## Using the Twitter API to implement sentiment analysis on different sets of stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1) Getting the Tweepy API working for Python\n",
    "\n",
    "* Import the dependencies for the rest of the project here, this section will be kept up-to-date\n",
    "* Create a twitter account (out of scope) and through the developer section, get a set of `OAuth` credentials\n",
    "* Instantiate the api and make some test searches\n",
    "* Define some helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Import the necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler, Stream, StreamListener\n",
    "tweepy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas_datareader.data as pdr\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.4'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config/keys.txt']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.read('config/keys.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Authenticate User: you should use ideally use your own login credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = config['DEFAULT']['consumer_key']\n",
    "consumer_secret = config['DEFAULT']['consumer_secret']\n",
    "access_token = config['DEFAULT']['access_token']\n",
    "access_secret = config['DEFAULT']['access_secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "# plug into the matrix\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Test the tweepy api with Microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_stock = 'Microsoft'\n",
    "test_ticker = 'MSFT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = api.search(q=test_stock, count=10, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-03 00:32:02\n",
      "@SavinTheBees Microsoft came out and basically said there are other consoles that people prefer over Xbox, so they’… https://t.co/gFP3EuZ3IT\n",
      "2019-04-03 00:32:01\n",
      "Microsoft stops selling ebooks and will refund customers for previous purchases\n",
      "2019-04-03 00:31:58\n",
      "@NatashaVianna @CharlottePigg Save your allowance and buy IBM, Microsoft, Apple, Amazon, and Google stocks.\n"
     ]
    }
   ],
   "source": [
    "for s in search_results[:3]:\n",
    "    print(s.created_at)\n",
    "    print(s.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Define helper functions:\n",
    "\n",
    "* One to get the corpus from a tweet\n",
    "* One to perform a twitter search with a string and collect 100 tweets before a given date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_corpus(status):\n",
    "    \"\"\"\n",
    "    Given a tweepy.models.Status object, returns the corpus as a str object\n",
    "    \n",
    "    :params: status, tweepy.models.Status\n",
    "    :returns: corpus, str\n",
    "    \"\"\"\n",
    "    if isinstance(status, tweepy.models.Status):\n",
    "        return status.text\n",
    "    else:\n",
    "        raise TypeError(\"Input not of type tweepy.api.Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_tweets(query, \n",
    "                   limit=1000,\n",
    "                   dt=datetime.datetime.now(),\n",
    "                   tz='US/Eastern'):\n",
    "    \n",
    "    assert(isinstance(query, str))\n",
    "\n",
    "    local_tz = pytz.timezone(tz)\n",
    "    local_dt = local_tz.localize(dt)\n",
    "    \n",
    "        \n",
    "    valid_results = []\n",
    "    for s in tweepy.Cursor(api.search, q=query, rpp=10,count=100, lang='en').items(limit):\n",
    "        if local_tz.localize(s.created_at) < local_dt:\n",
    "            \n",
    "            valid_results.append(s)\n",
    "            \n",
    "    if len(valid_results) < 100:\n",
    "        print(\"WARN: Less than 100 results, you should be using expanded_search()\")\n",
    "        \n",
    "    return valid_results[:100]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = collect_tweets('GRANITE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Man, planks have destroyed my elbows. One would think I was doing them on granite. Sigh'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Determining the Winners and Losers for a given day\n",
    "\n",
    "* Define the scope of the problem. Here, I have chosen the NASDAQ index and have pulled a .csv file of the companies\n",
    "* Define functions to find the winners and losers, winners and losers are defined by their diff = price_close - price_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 4, 2, 20, 32, 28, 421016)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Read in all companies on the NASDAQ, I have pre-populated a list of the companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticker_df = pd.read_csv('files\\companylist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Symbol', 'Name', 'LastSale', 'MarketCap', 'IPOyear', 'Sector',\n",
       "       'industry', 'Summary Quote', 'Unnamed: 8'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Name</th>\n",
       "      <th>LastSale</th>\n",
       "      <th>MarketCap</th>\n",
       "      <th>IPOyear</th>\n",
       "      <th>Sector</th>\n",
       "      <th>industry</th>\n",
       "      <th>Summary Quote</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YI</td>\n",
       "      <td>111, Inc.</td>\n",
       "      <td>6.5100</td>\n",
       "      <td>$530.85M</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Medical/Nursing Services</td>\n",
       "      <td>https://www.nasdaq.com/symbol/yi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PIH</td>\n",
       "      <td>1347 Property Insurance Holdings, Inc.</td>\n",
       "      <td>5.2899</td>\n",
       "      <td>$31.81M</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Property-Casualty Insurers</td>\n",
       "      <td>https://www.nasdaq.com/symbol/pih</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PIHPP</td>\n",
       "      <td>1347 Property Insurance Holdings, Inc.</td>\n",
       "      <td>24.5000</td>\n",
       "      <td>$17.15M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Property-Casualty Insurers</td>\n",
       "      <td>https://www.nasdaq.com/symbol/pihpp</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TURN</td>\n",
       "      <td>180 Degree Capital Corp.</td>\n",
       "      <td>1.8600</td>\n",
       "      <td>$57.89M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Finance/Investors Services</td>\n",
       "      <td>https://www.nasdaq.com/symbol/turn</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FLWS</td>\n",
       "      <td>1-800 FLOWERS.COM, Inc.</td>\n",
       "      <td>18.3400</td>\n",
       "      <td>$1.18B</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Consumer Services</td>\n",
       "      <td>Other Specialty Stores</td>\n",
       "      <td>https://www.nasdaq.com/symbol/flws</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol                                    Name  LastSale MarketCap  IPOyear  \\\n",
       "0     YI                               111, Inc.    6.5100  $530.85M   2018.0   \n",
       "1    PIH  1347 Property Insurance Holdings, Inc.    5.2899   $31.81M   2014.0   \n",
       "2  PIHPP  1347 Property Insurance Holdings, Inc.   24.5000   $17.15M      NaN   \n",
       "3   TURN                180 Degree Capital Corp.    1.8600   $57.89M      NaN   \n",
       "4   FLWS                 1-800 FLOWERS.COM, Inc.   18.3400    $1.18B   1999.0   \n",
       "\n",
       "              Sector                    industry  \\\n",
       "0        Health Care    Medical/Nursing Services   \n",
       "1            Finance  Property-Casualty Insurers   \n",
       "2            Finance  Property-Casualty Insurers   \n",
       "3            Finance  Finance/Investors Services   \n",
       "4  Consumer Services      Other Specialty Stores   \n",
       "\n",
       "                         Summary Quote  Unnamed: 8  \n",
       "0     https://www.nasdaq.com/symbol/yi         NaN  \n",
       "1    https://www.nasdaq.com/symbol/pih         NaN  \n",
       "2  https://www.nasdaq.com/symbol/pihpp         NaN  \n",
       "3   https://www.nasdaq.com/symbol/turn         NaN  \n",
       "4   https://www.nasdaq.com/symbol/flws         NaN  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       YI\n",
       "1      PIH\n",
       "2    PIHPP\n",
       "3     TURN\n",
       "4     FLWS\n",
       "Name: Symbol, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers = ticker_df['Symbol']\n",
    "tickers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# useful datetime variable\n",
    "start_dt = datetime.datetime(2019, 3, 27)\n",
    "end_dt = start_dt + datetime.timedelta(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) define three helper functions: \n",
    "\n",
    "* One to compute the stock's gain or loss for a given day\n",
    "* One to compile a dictionary of all (or some) of the NASDAQs stocks' gains/losses for a given day.\n",
    "By default, the limit is set at 50 out of the 3500 or so NASDAQ companies. Simply set limit=None to\n",
    "scan the entire NASDAQ index\n",
    "* One to identify the winners and losers given the dictionary and return a winner's dictionary and a loser's\n",
    "dictionary based on the differential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_diff(df):\n",
    "    assert(df.shape[0] == 1)\n",
    "    return float(df['close'] - df['open']) / float(df['open'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_diffs(tickers, \n",
    "                  limit=50, \n",
    "                  dt=datetime.datetime.now() - datetime.timedelta(1)):\n",
    "    \n",
    "    if limit and limit < len(tickers):\n",
    "        _tickers = tickers[:limit]\n",
    "    else:\n",
    "        _tickers = tickers\n",
    "    \n",
    "    _diffs = dict()\n",
    "    \n",
    "    for ticker in _tickers:\n",
    "    \n",
    "        try:\n",
    "            _df = pdr.DataReader(ticker, 'iex', dt, dt)\n",
    "            diff_value = _get_diff(_df)\n",
    "            _diffs[ticker] = diff_value\n",
    "        except Exception as e:\n",
    "            print(ticker, _df.shape)\n",
    "            \n",
    "    return _diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_gainers_and_losers(diff_dict):\n",
    "    \n",
    "    _df = pd.DataFrame([diff_dict.keys(), diff_dict.values()]).T\n",
    "    _df.dropna(axis=0, inplace=True)\n",
    "    _df.columns = ['ticker', 'diff']\n",
    "    _df.sort_values('diff', inplace=True, ascending=False)\n",
    "    _df.set_index('ticker', inplace=True, drop=True)\n",
    "    \n",
    "    winners = _df.iloc[:3, :]\n",
    "    losers = _df.iloc[-3:, :]\n",
    "    \n",
    "    return winners.to_dict()['diff'], losers.to_dict()['diff']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JFKKU (0, 5)\n",
      "ADILW (0, 5)\n",
      "ALACR (0, 5)\n",
      "ALACU (0, 5)\n",
      "ALACW (0, 5)\n",
      "ALIT (1, 5)\n",
      "ALGRR (0, 5)\n",
      "ALGRU (0, 5)\n",
      "SMCP (0, 5)\n",
      "AMCIU (0, 5)\n",
      "AMCIW (0, 5)\n"
     ]
    }
   ],
   "source": [
    "diffs = get_all_diffs(tickers, 200, start_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gainers, losers = find_gainers_and_losers(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABEOW': 0.08098591549295775,\n",
       " 'AMRS': 0.20618556701030924,\n",
       " 'AMRWW': 0.8095238095238093}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AGFSW': -0.1666666666666666,\n",
       " 'AKAO': -0.0758823529411765,\n",
       " 'AMRB': -0.0825057295645531}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Combining the previous sections\n",
    "\n",
    "Now that we have achieved the basic abilities to use the tweepy API and the pandas data reader, we want to abstract\n",
    "their functionality into a more structured piece of software with clear inputs and outputs and robust parameter handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### We need to define a function that will take in the corpus and return a DataFrame with all the desired fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def assemble_corpus(tweet_list):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can define the expanded_search() function here to ensure we have 100 tweets per company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expanded_search(ticker, df):\n",
    "    \n",
    "    alt_terms = df[df.loc[:, 'Symbol'] == ticker].loc[:, ['Symbol', 'Name', 'Sector']].values.tolist()[0]\n",
    "    \n",
    "    valid_results = []\n",
    "    for term in alt_terms:\n",
    "        _sub_results = collect_tweets(term)\n",
    "        valid_results.extend(_sub_results)\n",
    "        \n",
    "        if len(valid_results) > 100:\n",
    "            return valid_results[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Less than 100 results, you should be using expanded_search()\n",
      "WARN: Less than 100 results, you should be using expanded_search()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_list = expanded_search('AMRB', ticker_df)\n",
    "type(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [t.text for t in tweet_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$0.25 EPS Expected for American River Bankshares $AMRB  https://t.co/mI1NOxJiNt'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = texts[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._text = None\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        \n",
    "        self._text = self._remove_https_tag(text)\n",
    "        self._text = self._tokenize(self._text)\n",
    "        self._text = self._lemmatize(self._text)\n",
    "        \n",
    "        self._text = self._lower(self._text)\n",
    "        self._text = self._remove_symbols(self._text)\n",
    "        return self._text\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def _remove_https_tag(raw):\n",
    "        return re.sub('https://[\\w\\.\\/]+', '', raw).strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _tokenize(raw):\n",
    "        return nltk.word_tokenize(raw)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _stem(raw_tokens):\n",
    "        porter = nltk.PorterStemmer()\n",
    "        return [porter.stem(t) for t in raw_tokens]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _lemmatize(raw_tokens):\n",
    "        wnl = nltk.WordNetLemmatizer()\n",
    "        return [wnl.lemmatize(t) for t in raw_tokens]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _lower(raw_tokens):\n",
    "        return [t.lower() for t in raw_tokens]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _remove_nonwords(raw_tokens):\n",
    "        return [t for t in raw_tokens if t in nltk.corpus.words.words('en')]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _remove_symbols(raw_tokens):\n",
    "        # bad_char = ['`','~','!','@','#','$','%','^','&','*','(',')','-','_','+','=','/','\\\\']\n",
    "        _t = [t for t in raw_tokens if not re.match(\"\\d+\\.?\\d+\", t)]\n",
    "        _t = [t for t in _t if t.isalpha()]\n",
    "        return _t\n",
    "\n",
    "    @staticmethod\n",
    "    def _remove_stopwords(raw_tokens):\n",
    "        return [t for t in raw_tokens if not t in nltk.corpus.stopwords.words('english')]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = preprocessor.process_text(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$0.25 EPS Expected for American River Bankshares $AMRB  https://t.co/mI1NOxJiNt'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
