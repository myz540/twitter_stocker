{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSS688 Term Project Option 3\n",
    "# Twitter Stocker\n",
    "## Author: Mike Zhong\n",
    "## Using the Twitter API to implement sentiment analysis on different sets of stocks\n",
    "## https://github.com/myz540/twitter_stocker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "* Create a twitter account (out of scope) and through the developer section, get a set of `OAuth` credentials\n",
    "* `import` dependencies (see `requirements.txt` for specific version numbers)\n",
    "* The `requirements.txt` file has a LONG list of requirements, this is a result of my using an anaconda distribution and using `pip freeze > requirements.txt` which inevitably captured all the packages bundled with the distribution\n",
    "* `import twitter_utils`, my custom library containing classes to handle the work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import the necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# comes with python 3.6\n",
    "import datetime\n",
    "import re\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# additional libraries\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is my custom module and contains classes which implement all the heavy lifting. It merits a good read and implements a\n",
    "# (hopefully) easy-to-use interface for developers to build off of.\n",
    "from twitter_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config/keys.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.read('config/keys.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Authenticate User: you should use ideally use your own login credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = config['DEFAULT']['consumer_key']\n",
    "consumer_secret = config['DEFAULT']['consumer_secret']\n",
    "access_token = config['DEFAULT']['access_token']\n",
    "access_secret = config['DEFAULT']['access_secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tweepy.api.API at 0x26647bd1c88>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "# plug into the matrix\n",
    "api = tweepy.API(auth)\n",
    "api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Load the pre-populated list of NASDAQ companies\n",
    "This csv file was downloaded from the internet via a simple Google search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticker_df = pd.read_csv('files/companylist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Name</th>\n",
       "      <th>LastSale</th>\n",
       "      <th>MarketCap</th>\n",
       "      <th>IPOyear</th>\n",
       "      <th>Sector</th>\n",
       "      <th>industry</th>\n",
       "      <th>Summary Quote</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YI</td>\n",
       "      <td>111, Inc.</td>\n",
       "      <td>6.5100</td>\n",
       "      <td>$530.85M</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Medical/Nursing Services</td>\n",
       "      <td>https://www.nasdaq.com/symbol/yi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PIH</td>\n",
       "      <td>1347 Property Insurance Holdings, Inc.</td>\n",
       "      <td>5.2899</td>\n",
       "      <td>$31.81M</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Property-Casualty Insurers</td>\n",
       "      <td>https://www.nasdaq.com/symbol/pih</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PIHPP</td>\n",
       "      <td>1347 Property Insurance Holdings, Inc.</td>\n",
       "      <td>24.5000</td>\n",
       "      <td>$17.15M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Property-Casualty Insurers</td>\n",
       "      <td>https://www.nasdaq.com/symbol/pihpp</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TURN</td>\n",
       "      <td>180 Degree Capital Corp.</td>\n",
       "      <td>1.8600</td>\n",
       "      <td>$57.89M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Finance/Investors Services</td>\n",
       "      <td>https://www.nasdaq.com/symbol/turn</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FLWS</td>\n",
       "      <td>1-800 FLOWERS.COM, Inc.</td>\n",
       "      <td>18.3400</td>\n",
       "      <td>$1.18B</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Consumer Services</td>\n",
       "      <td>Other Specialty Stores</td>\n",
       "      <td>https://www.nasdaq.com/symbol/flws</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol                                    Name  LastSale MarketCap  IPOyear  \\\n",
       "0     YI                               111, Inc.    6.5100  $530.85M   2018.0   \n",
       "1    PIH  1347 Property Insurance Holdings, Inc.    5.2899   $31.81M   2014.0   \n",
       "2  PIHPP  1347 Property Insurance Holdings, Inc.   24.5000   $17.15M      NaN   \n",
       "3   TURN                180 Degree Capital Corp.    1.8600   $57.89M      NaN   \n",
       "4   FLWS                 1-800 FLOWERS.COM, Inc.   18.3400    $1.18B   1999.0   \n",
       "\n",
       "              Sector                    industry  \\\n",
       "0        Health Care    Medical/Nursing Services   \n",
       "1            Finance  Property-Casualty Insurers   \n",
       "2            Finance  Property-Casualty Insurers   \n",
       "3            Finance  Finance/Investors Services   \n",
       "4  Consumer Services      Other Specialty Stores   \n",
       "\n",
       "                         Summary Quote  Unnamed: 8  \n",
       "0     https://www.nasdaq.com/symbol/yi         NaN  \n",
       "1    https://www.nasdaq.com/symbol/pih         NaN  \n",
       "2  https://www.nasdaq.com/symbol/pihpp         NaN  \n",
       "3   https://www.nasdaq.com/symbol/turn         NaN  \n",
       "4   https://www.nasdaq.com/symbol/flws         NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3428, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Determine the Gainers and Losers for a given day\n",
    "The `StockHandler` object from `twitter_utils` is a custom object implemented to find the gainers and losers from a given day. The object wraps the `pandas_datareader` and makes calls to the `iex` financial database in order to get stock price information. The `StockHandler` also implements methods for computing the gain/loss or `diff`, as well as finding the three winning and losing stocks for a given day.\n",
    "\n",
    "There are several matters to address here:\n",
    "\n",
    "1) How do we define a gain or a loss?\n",
    "\n",
    "* A gain or loss will be calculated as price(close)- price(open) / price(open)\n",
    "\n",
    "2) What day should we use when finding the gainers and losers?\n",
    "\n",
    "* technically, any day can be passed to the StockHandler method for collecting tweets, I will use the yesterday\n",
    "\n",
    "3) How can we ensure the tweets fetched are relevant to the given day?\n",
    "\n",
    "* To ensure the tweets fetched were tweeted before the day in question, the implementing methods will check the timestamp\n",
    "of the tweet to ensure it is before the day in question. The tweets are returned in order of \"most recent\" so we are sure \n",
    "to capture relevant tweets\n",
    "\n",
    "To accomplish this, we create a dictionary with the company's ticker as the key and the `diff` as the value, where the `diff` is computed as stated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       YI\n",
      "1      PIH\n",
      "2    PIHPP\n",
      "3     TURN\n",
      "4     FLWS\n",
      "Name: Symbol, dtype: object\n",
      "3428\n"
     ]
    }
   ],
   "source": [
    "tickers = ticker_df['Symbol'].drop_duplicates()\n",
    "print(tickers.head())\n",
    "print(len(tickers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the date of the lookup will default to yesterday\n",
    "# this step is lengthy since the pandas_datareader is limited in the number of queries it can make per unit time\n",
    "# for demo purposes, I've used a reduce limit by uncommenting/commenting\n",
    "\n",
    "limit = 100\n",
    "#limit = len(tickers)\n",
    "diff_dict = StockHandler.get_all_diffs(tickers, limit=limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GNMX': 0.20399999999999996, 'AGFSW': 0.16749999999999993, 'AKTX': 0.04985337243401757, 'ADIL': 0.04382470119521926, 'AMCN': 0.03296703296703287, 'ADAP': 0.03225806451612891, 'AIRG': 0.030322580645161332, 'ADRO': 0.03022670025188908, 'ARPO': 0.01960784313725492, 'ABMD': 0.01611662576462413}\n",
      "{'AMTX': -0.04285714285714277, 'ACAD': -0.04449741756058804, 'ARAY': -0.04827586206896551, 'ABEO': -0.05637254901960784, 'YI': -0.057803468208092484, 'ACOR': -0.06025179856115108, 'ADES': -0.06611570247933876, 'ACRS': -0.08029197080291968, 'AKRX': -0.08783783783783777, 'ADOM': -0.13053401609363574}\n"
     ]
    }
   ],
   "source": [
    "winner_dict, loser_dict = StockHandler.find_gainers_and_losers(diff_dict)\n",
    "print(winner_dict)\n",
    "print(loser_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Instantiate a CorpusHandler and begin querying twitter\n",
    "Now we know the companies we are interested in, we can start querying twitter. In my first pass, I created a search function to query just the ticker symbol, but found that I often could not find 100 tweets for small, irrelevant companies, which sometimes show up as winners or losers. To guard against this, I created an `extended_search` method which will also query the company name, and finally, the sector, if the symbol alone doesn't provide enough tweets\n",
    "\n",
    "It is important to note that the search period for these tweets must be relevant to the when the gainers and losers were identified. The `expanded_search` and `collect_tweets` methods both default to yesterday\n",
    "\n",
    "The `CorpusHandler` object from `twitter_utils` is a custom object that was implemented to handle much of the heavy lifting. This class contains a variety of methods for handling corpus as strings of tokens separated by whitespace or a delimiter of your choice, as well as converting the corpus into a list of tokens. This class also implements methods for saving and loading a corpora. The two attributes `gainer_corpus` and `loser_corpus` are populated when read from disk, these objects are also what get written to disk when saving. They are `dict` objects with the company as the key mapping to the 100 tweets stored as a `list` of strings. These strings will be pre-processed in section 6 before ultimately populating these two attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a CorpusHandler, custom object \n",
    "corpus_handler = CorpusHandler(api, ticker_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNMX\n",
      "WARN: Less than 100 results, you should be using expanded_search()\n",
      "100\n",
      "AGFSW\n",
      "WARN: Less than 100 results, you should be using expanded_search()\n",
      "WARN: Less than 100 results, you should be using expanded_search()\n",
      "WARN: Less than 100 results, you should be using expanded_search()\n",
      "AKTX\n",
      "WARN: Less than 100 results, you should be using expanded_search()\n",
      "100\n",
      "ADIL\n"
     ]
    },
    {
     "ename": "TweepError",
     "evalue": "Twitter error response: status code = 429",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTweepError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2e7c300ef667>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msymbol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwinner_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtweet_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpanded_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtweet_list\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mike Zhong\\Dropbox\\School\\CS688 Web Analytics and Mining\\twitter-stocker\\twitter_utils.py\u001b[0m in \u001b[0;36mexpanded_search\u001b[0;34m(self, ticker)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mvalid_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malt_terms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0m_sub_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mvalid_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sub_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mike Zhong\\Dropbox\\School\\CS688 Web Analytics and Mining\\twitter-stocker\\twitter_utils.py\u001b[0m in \u001b[0;36mcollect_tweets\u001b[0;34m(self, query, limit, dt, tz)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mvalid_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrpp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlocal_tz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreated_at\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlocal_dt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mvalid_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[1;31m# Reached end of current page, get the next page...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__self__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[1;31m# Set pagination mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    227\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                     \u001b[1;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[1;31m# Parse the response payload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTweepError\u001b[0m: Twitter error response: status code = 429"
     ]
    }
   ],
   "source": [
    "# we need 3 companies that can produce 100 tweets, we have 10 gainers and losers so that should be sufficient search space\n",
    "# it's easy to overload the twitter API and go over the usage limit...\n",
    "gainer_tweet_dict = dict()\n",
    "good_tweets = 0\n",
    "\n",
    "for symbol in winner_dict.keys():\n",
    "    print(symbol)\n",
    "    tweet_list = corpus_handler.expanded_search(symbol)\n",
    "    \n",
    "    if tweet_list and len(tweet_list) == 100:\n",
    "        print(len(tweet_list))\n",
    "        gainer_tweet_dict[symbol] = tweet_list \n",
    "        good_tweets += 1\n",
    "        \n",
    "    if good_tweets >= 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loser_tweet_dict = dict()\n",
    "good_tweets = 0\n",
    "\n",
    "for symbol in loser_dict.keys():\n",
    "    print(symbol)\n",
    "    tweet_list = corpus_handler.expanded_search(symbol)\n",
    "    \n",
    "    if tweet_list and len(tweet_list) == 100:\n",
    "        print(len(tweet_list))\n",
    "        good_tweets += 1\n",
    "        loser_tweet_dict[symbol] = tweet_list \n",
    "    \n",
    "    if good_tweets >= 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Pre Process the tweets such that each tweet ends up as a clean string of tokens separated by whitespace.\n",
    "1) Take the tweet, which is a `tweepy.api.Status` object and convert it to a `str` using our helper function\n",
    "\n",
    "2) Use the `PreProcessor` object to clean and tokenize the string into a `list` of tokens\n",
    "\n",
    "3) Convert the `list` of tokens back into a string of tokens separated by white space\n",
    "\n",
    "4) Populate the gainer and loser corpus using the symbol as the key and the list of cleaned tweets as the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate the PreProcessor\n",
    "preprocessor = PreProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_handler.gainer_corpus = dict()\n",
    "corpus_handler.loser_corpus = dict()\n",
    "\n",
    "print(\"Populating gainer_corpus\")\n",
    "for symbol, tweet_list in gainer_tweet_dict.items():\n",
    "    corpus_handler.gainer_corpus[symbol] = list()\n",
    "    for tweet in tweet_list:\n",
    "        text = corpus_handler.get_corpus(tweet)\n",
    "        cleaned_tokens = preprocessor.process_text_tweet(text)\n",
    "        cleaned_text = corpus_handler.convert_list_to_corpus(cleaned_tokens)\n",
    "        corpus_handler.gainer_corpus[symbol].append(cleaned_text)\n",
    "    \n",
    "print(\"Populating loser_corpus\")\n",
    "for symbol, tweet_list in loser_tweet_dict.items():\n",
    "    corpus_handler.loser_corpus[symbol] = list()\n",
    "    for tweet in tweet_list:\n",
    "        text = corpus_handler.get_corpus(tweet)\n",
    "        cleaned_tokens = preprocessor.process_text_tweet(text)\n",
    "        cleaned_text = corpus_handler.convert_list_to_corpus(cleaned_tokens)\n",
    "        corpus_handler.loser_corpus[symbol].append(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7) We can now find the most commonly used words in each corpus and visualize. We can also define a vocabulary using these words and subsequently create document-term matrices which we can write and read from disk as numpy objects\n",
    "1) Use the `CorpusHandler.get_word_counts()` method to get the most commonly used words\n",
    "\n",
    "2) Use `wordcloud.Wordcloud()` to assist with visualization. Wrapped by the `CorpusHandler.make_word_cloud()` method\n",
    "\n",
    "3) We can save the corpora stored in the `CorpusHandler.gainer_corpus` and `CorpusHandler.loser_corpus` using the `save_corpus` method\n",
    "\n",
    "4) we can load corpora into the `CorpusHandler.gainer_corpus` and `CorpusHandler.loser_corpus` using the `load_corpus` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gainer_words = list()\n",
    "loser_words = list()\n",
    "\n",
    "for k,v in corpus_handler.gainer_corpus.items():\n",
    "    gainer_words = [words.split() for words in v]\n",
    "    gainer_words = [word for words in gainer_words for word in words]\n",
    "    \n",
    "\n",
    "for k,v in corpus_handler.loser_corpus.items():\n",
    "    loser_words = [words.split() for words in v]\n",
    "    loser_words = [word for words in loser_words for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(gainer_words))\n",
    "print(len(loser_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gainer_df = CorpusHandler.get_word_counts(gainer_words)\n",
    "gainer_df.sort_values('frequency', ascending=False, inplace=True)\n",
    "gainer_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loser_df = CorpusHandler.get_word_counts(loser_words)\n",
    "loser_df.sort_values('frequency', ascending=False, inplace=True)\n",
    "loser_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CorpusHandler.make_wordcloud(gainer_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CorpusHandler.make_wordcloud(loser_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the corpora, my convention prefixes 'gainer.' and 'loser.' to the argument\n",
    "# any crazy characters that can't be encoded will throw a `UnicodeEncodeError` and those tweets will be skipped\n",
    "corpus_handler.save_corpus(filename='data.corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can load the corpora using the command below, just uncomment\n",
    "\n",
    "# corpus_hander.load_corpus(filename='data.corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 8) Vocabulary and Document-Term Matrix\n",
    "1) Define a vocab using the 100 most commonly used words from each corpus\n",
    "* much of this work is done in Section 7\n",
    "\n",
    "2) Create dtms using this vocab for the two corpora\n",
    "* We will use the `sklearn.feature_extraction.text.CountVectorizer()` object and can generate sparse matrices representing a dtm by instantiating the object with a vocab generated earlier, and then passing each corpus through it. Each row will end up being a tweet\n",
    "* encode gainers as 0 and losers as 1\n",
    "\n",
    "3) We can proceed to train a model with this dtm but that is beyond the scope of this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gainer_most_common = gainer_df.loc[:, 'word'][:100].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loser_most_common = loser_df.loc[:, 'word'][:100].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_common = set(gainer_most_common + loser_most_common)\n",
    "len(total_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=total_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_gainer_tweets = [v for vs in corpus_handler.gainer_corpus.values() for v in vs]\n",
    "all_loser_tweets = [v for vs in corpus_handler.loser_corpus.values() for v in vs]\n",
    "print(len(all_gainer_tweets))\n",
    "print(len(all_loser_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = all_gainer_tweets + all_loser_tweets\n",
    "all_y = [0]*300 + [1]*300\n",
    "len(all_tweets), len(all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit_transform(all_tweets, all_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Sentiment Analysis using VADER sentiment analyzer\n",
    "The VADER, or the Valence Aware Dictionary and sEntiment Reasoner is a python package with a pre-trained sentiment analyzer. Although the implementation details of this package were not personally investigated by me, the literature suggests this is a reliable tool, though my pre-processing probably weakens its effectiveness since it makes good use of emojis and other acronyms and memes which have invaded our lexicon\n",
    "\n",
    "The `CorpusHandler.analyze_sentiment` method takes a list of words and returns it's sentiment. In our case, we have that list of words already from Section 7 and can re-use it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gainer_sentiment = corpus_handler.analyze_sentiment(gainer_words)\n",
    "gainer_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".191 / .068"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loser_sentiment = corpus_handler.analyze_sentiment(loser_words)\n",
    "loser_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".141 / .084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the results show VADER does not segregate the two corpora well at all. Discarding the `compound` category, we can see the positive goes from .191 to .141 and the negative from .068 to 0.84. If we take the ratio of `pos`/`neg` and use that as our primary statistic, we get values of 2.8 and 1.7 respectively. If we think a larger ratio as being more positive and a smaller (ideally less than 1) value as being negative, we could potentially use as a feature in any model training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
